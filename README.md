# üöÄ Implanta√ß√£o de Servi√ßos de Alta Disponibilidade na AWS

Este reposit√≥rio cont√©m a Arquitetura de Refer√™ncia e o c√≥digo completo para a implanta√ß√£o automatizada de servi√ßos de alta disponibilidade na AWS, utilizando **EKS (Kubernetes)** e a metodologia **IaC (Infrastructure as Code)**.

A solu√ß√£o implementa todos os requisitos m√≠nimos e avan√ßa nos componentes b√≥nus

## 1. üó∫Ô∏è Arquitetura e Fluxo da Solu√ß√£o

A arquitetura √© projetada para o Zero Trust (seguran√ßa rigorosa) e para a recupera√ß√£o autom√°tica.

### 1.1. Fluxo de Tr√°fego do Usu√°rio (Entrada)

Esta √© a jornada de uma requisi√ß√£o para a aplica√ß√£o (`whoami`):

1. **Usu√°rio** acessa `https://bry.jotinha.dev`.
2. **AWS Route 53** (gerenciado automaticamente pelo **ExternalDNS**) aponta para o NLB.
3. **AWS Network Load Balancer (NLB)** (Criado pelo Nginx Ingress) encaminha o tr√°fego.
4. **Nginx Ingress Controller** (Rodando no EKS) **termina o SSL** (usando o certificado do **Cert-Manager**) e roteia a requisi√ß√£o para o Service (`whoami-service`).
5. **Aplica√ß√£o `whoami`** (Rodando em Nodes `t3.large` privados) responde.

### 1.2. Fluxo de Gerenciamento (CI/CD Automatizado)

O gerenciamento √© totalmente desacoplado e automatizado via **GitHub Actions** (OIDC):

- **Trilha de Infraestrutura:** Mudan√ßas na pasta `/terraform` disparam o pipeline de Infra. O rob√¥ veste o **IAM Role de Administrador (OIDC)**, executa `terraform plan`, e depois `terraform apply`.
- **Trilha da Aplica√ß√£o:** Mudan√ßas no c√≥digo/manifestos disparam um pipeline (n√£o implementado neste commit, mas planejado) que constr√≥i o Docker, faz `push` para o ECR e atualiza o Helm no EKS.

---

## 2. üõ°Ô∏è Pilares da Alta Disponibilidade (HA) e Seguran√ßa

Os seguintes componentes foram implementados e configurados para garantir a resili√™ncia e a seguran√ßa avan√ßada:

| Pilar | Ferramenta | Justificativa |
| --- | --- | --- |
| **Infraestrutura HA** | **AWS EKS + Terraform** | O Control Plane √© gerenciado pela AWS. Os Worker Nodes (`t3.large`) s√£o distribu√≠dos em M√∫ltiplas Zonas de Disponibilidade (Multi-AZ) para resili√™ncia a falhas de datacenter. |
| **Isolamento de Rede** | **AWS VPC + Subnets Privadas** | Os Nodes que rodam a aplica√ß√£o est√£o em Subnets Privadas e s√≥ podem ser alcan√ßados atrav√©s do NLB. |
| **Zero Trust** | **Calico Network Policies** | Aplic√°mos a regra `default-deny` no namespace principal, bloqueando todo o tr√°fego interno. Isso impede que um pod invadido se mova lateralmente pelo cluster. |
| **Escalabilidade** | **Kubernetes HPA** | Configur√°mos o `whoami-hpa` para dimensionar de 2 para 10 r√©plicas automaticamente com base na utiliza√ß√£o da CPU (`averageUtilization: 80%`). |
| **Escalabilidade (B√≥nus)** | **KEDA (SQS)** | Configur√°mos o `ScaledObject` para o `whoami-deployment` ligar-se √† Fila SQS (`jotinha-whoami-jobs`) e escalar de **0 para N** (Scale-to-Zero), otimizando os custos em momentos de inatividade. |
| **Gerenciamento de Segredos** | **HashiCorp Vault + Injector** | Instalamos o Vault em modo `dev` e configur√°mos o Ingress. O Vault Agent Injector injeta os segredos como um *sidecar* diretamente no pod, evitando a exposi√ß√£o de senhas em Secrets do Kubernetes. |

---

### **2.1. üíæ Os Componentes de Infraestrutura como C√≥digo (IaC)**

A arquitetura √© constru√≠da com **separa√ß√£o total** de responsabilidades para evitar "Desvios" (Drift) e garantir a rastreabilidade.**ComponenteFerramenta/Localiza√ß√£oProp√≥sito ArquiteturalAcesso Web**AWS NLB (Layer 4)Fornece um √∫nico ponto de entrada **el√°stico** e de baixo custo para o cluster.**Roteamento SSL**Nginx Ingress + Cert-ManagerO Nginx atua como **Recepcionista** (roteador Layer 7) e termina o SSL usando certificados Let's Encrypt.**Persist√™ncia de Aplica√ß√£o**EBS Volumes (`gp2`/`gp3`)Volumes de disco r√°pidos anexados aos Nodes (`t3.large`) para o uso do Elasticsearch.**Provedor OIDC**AWS IAMCria a rela√ß√£o de **confian√ßa criptogr√°fica** para o CI/CD (GitHub) e para os Pods (IRSA).**Worker Nodes**AWS EC2 (`t3.large`)Fornece os recursos de c√¥mputo (8GB RAM) necess√°rios para rodar cargas pesadas como a pilha de Logs.

### **2.2. üîí A Tr√≠ade da Seguran√ßa e da Confian√ßa**

Estes componentes s√£o os guardi√µes da estabilidade e do acesso.

**A. O "Policial de Tr√°fego" (Calico)**

‚Ä¢ **O que √©:** O CNI (Container Network Interface) e motor de Network Policy. O Calico √© a ferramenta que nos permitiu implementar a filosofia **Zero Trust** dentro do cluster.
‚Ä¢ **Implementa√ß√£o:** Instalado via Helm. Por defeito, o Calico **bloqueia toda a comunica√ß√£o pod-a-pod** entre Namespaces.
‚Ä¢ **Regra Cr√≠tica:** Foi necess√°rio adicionar a regra `allow-ingress-nginx.yaml` para explicitamente permitir que o **Recepcionista** (`ingress-nginx` namespace) falasse com os **Trabalhadores** (`default` namespace). Sem esta regra, o Cert-Manager n√£o conseguiria completar a prova SSL, e a aplica√ß√£o seria inacess√≠vel.

**B. O "Cofre" e o "Cadeado" (S3 & DynamoDB)**

‚Ä¢ **AWS S3 Bucket (`jotinha-dev-terraform-state-prod...`)**
    ‚ó¶ **Prop√≥sito:** Serve como **Cofre** para o arquivo de estado (`terraform.tfstate`). Isto √© crucial para permitir o CI/CD (GitHub Actions) e a colabora√ß√£o de equipa, garantindo que a "mem√≥ria" da infraestrutura n√£o seja perdida ou fique no computador local de um engenheiro.
    ‚ó¶ **Seguran√ßa:** O Versionamento est√° ativo, permitindo *rollbacks* a vers√µes antigas do estado em caso de erro.
‚Ä¢ **AWS DynamoDB Table (`jotinha-dev-terraform-lock`)**
    ‚ó¶ **Prop√≥sito:** Serve como **Cadeado** (State Locking). A tabela impede que dois processos (ex: um engenheiro e o Rob√¥ do CI/CD) executem o `terraform apply` ao mesmo tempo, prevenindo a **corrup√ß√£o do estado** do projeto.

## 3. üíæ Estrutura do Projeto

O projeto segue a separa√ß√£o de responsabilidades (SoC):

```.
‚îú‚îÄ‚îÄ terraform/                # C√ìDIGO DA INFRAESTRUTURA (AWS)
‚îÇ   ‚îú‚îÄ‚îÄ main.tf               # Configura o backend S3, EKS e IAM
‚îÇ   ‚îî‚îÄ‚îÄ modules/
‚îÇ       ‚îú‚îÄ‚îÄ vpc/              # M√≥dulo isolado de rede (VPC, Subnets, NATs, EPs)
‚îÇ       ‚îî‚îÄ‚îÄ iam_roles/        # M√≥dulo que cria todos os IAM Roles (IRSA, CI/CD)
‚îú‚îÄ‚îÄ k8s/                      # MANIFESTOS DO KUBERNETES (YAMLS)
‚îÇ   ‚îú‚îÄ‚îÄ app/                  # Aplica√ß√£o (Deployment, Service, Ingress, HPA)
‚îÇ   ‚îú‚îÄ‚îÄ calico/               # Regras de Network Policy (Defesas internas)
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/           # Configura√ß√µes do Prometheus/Grafana
‚îÇ   ‚îî‚îÄ‚îÄ logging/              # Configura√ß√µes do Elasticsearch/Kibana
‚îî‚îÄ‚îÄ .github/
    ‚îî‚îÄ‚îÄ workflows/infra.yml   # O CI/CD (GitHub Actions)`
```
---

## 4. üìà Observabilidade Implementada (Ponto B√≥nus)

A pilha de Observabilidade foi projetada para cobrir Logs e M√©tricas:

| Componente | Fun√ß√£o | Localiza√ß√£o |
| --- | --- | --- |
| **Prometheus** | **M√©tricas e Alerting** (O "Fiscal") | Coleta o uso de CPU, mem√≥ria, lat√™ncia do Nginx e a sa√∫de geral do K8s. |
| **Grafana** | **Visualiza√ß√£o** (O "Painel de Gr√°ficos") | Apresenta os dashboards de forma gr√°fica. |
| **Elasticsearch** | **Armazenamento de Logs** (O "Arquivo Central") | Banco de dados centralizado que armazena os logs de todos os pods. |
| **Filebeat** | **Coleta de Logs** (O "Coletor") | Rodando em cada Node (`DaemonSet`), coleta logs e envia-os ao Elasticsearch. |

Para aceder ao Grafana ou Kibana (eles s√£o internos ao cluster), voc√™ deve usar o `kubectl port-forward` ap√≥s a implanta√ß√£o.

---

## 5. üõ†Ô∏è Primeiros Passos e Valida√ß√£o (Quick Start)

Assumindo que voc√™ tem as credenciais da AWS configuradas e os dom√≠nios `jotinha.dev` delegados ao Route 53.

### 5.1. Implanta√ß√£o da Infraestrutura (Terraform)

1. **Crie os Backends:** S3 Bucket (`jotinha-dev-terraform-state-prod-nova-conta`) e DynamoDB Table (`jotinha-dev-terraform-lock`).
2. **Inicialize:** Navegue at√© a pasta `/terraform` e execute:
    
    `terraform init`
    
3. **Construa:**
    
    `terraform apply`
    

### 5.2. Implanta√ß√£o dos Servi√ßos (Kubernetes)

1. **Conecte-se ao Cluster:**
    
    `aws eks update-kubeconfig --region us-east-1 --name jotinha-eks-cluster`
    
2. **Instale os M√≥dulos (Seguran√ßa e Aplica√ß√£o):**
    
    `helm install [o nome] [o chart]...
    kubectl apply -f k8s/calico/
    kubectl apply -f k8s/app/`
    

### 5.3. Valida√ß√£o de Ponta a Ponta

- **DNS & SSL:** O teste final √© o acesso externo, que confirma que todo o pipeline funcionou
    
    `https://bry.jotinha.dev`
    
- **Escalabilidade (HPA):** Verifique se o HPA est√° a funcionar e a vigiar a CPU
    
    `kubectl get hpa -n default`